{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo: Vectorización de textos\n",
    "**Autor:** Unidad de Científicos de Datos (UCD)\n",
    "\n",
    "---\n",
    "Este ejemplo muestra las principales funcionalidades del módulo `limpieza`, de la librería **ConTexto**. También se muestran ejemplos de uso de las funciones de limpieza contenidas en el módulo auxiliar `limpieza_aux`, que hace parte de `utils`.\n",
    "\n",
    "Para mayor información sobre estos módulos y sus funciones, se puede consultar [su documentación](link de la página de docs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Añadir la ubiación del paquete al path actual, para poder hacer las\n",
    "# importaciones\n",
    "scripts_path = os.path.abspath(os.path.join('../Contexto'))\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.insert(0, scripts_path)\n",
    "\n",
    "from contexto.limpieza import limpieza_texto, lista_stopwords\n",
    "from contexto.vectorizacion import *\n",
    "\n",
    "# Corpus de prueba\n",
    "textos_prueba = [\n",
    "    'Este es el primer texto de prueba para la vectorización y sus elementos.',\n",
    "    'Una segunda oración permite evaluar si hay elementos en común para vectorizar.',\n",
    "    'Tercera frase que consiste en un texto complementario con palabras comúnmente utilizadas.',\n",
    "    'En esta oración y la siguiente se introducen elementos para completar un grupo de por lo menos 5.',\n",
    "    'Finalmente, esta frase cierra un grupo de 5 oraciones para probar la vectorización.',\n",
    "    'Una última frase para ampliar un poco el grupo.']\n",
    "\n",
    "# Limpieza básica a los textos para quitar ruido\n",
    "textos_limpios = [limpieza_texto(i, lista_stopwords(), quitar_numeros=False) for i in textos_prueba]\n",
    "\n",
    "# Texto que no hace parte del corpus original\n",
    "texto_nuevo = 'hola, esta es una frase de prueba para aplicar la vectorización'\n",
    "\n",
    "### 1. Vectorizadores por frecuencia de términos ###\n",
    "\n",
    "## 1.1 Inicializar los vectorizadores\n",
    "v_bow = VectorizadorFrecuencias()\n",
    "# Este tiene en cuenta palabras y bigramas, y solo coge las 20 más frecuentes\n",
    "v_tfidf = VectorizadorFrecuencias('tfidf', rango_ngramas=(1, 2), max_elementos=20)\n",
    "\n",
    "## 1.2 Ajustar los vectorizadores\n",
    "# Se van a guardar los vectorizadores ajustados en archivos para su posterior uso\n",
    "v_bow.ajustar(textos_limpios, archivo_salida='out/v_bow.pk')\n",
    "v_tfidf.ajustar(textos_limpios, archivo_salida='out/v_tfidf.pk')\n",
    "\n",
    "## 1.3 Vocabulario de un vectorizador entrenado\n",
    "print(v_bow.vocabulario())\n",
    "print(v_tfidf.vocabulario())\n",
    "\n",
    "## 1.4 Vectorizar textos utilizando los vectorizadores entrenados\n",
    "vector_bow = v_bow.vectorizar(texto_nuevo, disperso=True)  # Salida como matriz dispersa\n",
    "vector_tfidf = v_tfidf.vectorizar(texto_nuevo, disperso=False)  # Salida como un numpy array\n",
    "\n",
    "## 1.5 Transformada inversa\n",
    "'''\n",
    "Nótese que al realizar la transformada inversa se pierde el orden de las palabras.\n",
    "Esto se debe a que estos métodos de vectorización no tienen en cuenta el orden\n",
    "sino la frecuencia de aparición de cada término.\n",
    "Además, si un término no está en el vocabulario del vectorizador, no va a estar\n",
    "incluído en el vector y por lo tanto no se podrá recuperar en la transformada\n",
    "inversa.\n",
    "'''\n",
    "print(textos_limpios[0])\n",
    "print(v_bow.inversa(v_bow.vectorizar(textos_prueba))[0])\n",
    "\n",
    "print(textos_limpios[2])\n",
    "print(v_tfidf.inversa(v_tfidf.vectorizar(textos_prueba))[2])\n",
    "\n",
    "## 1.6 Cargar un vectorizador ajustado previamente\n",
    "'''\n",
    "Al cargar un vectorizador ajustado previamente (mediante el parámetro \"archivo_modelo\")\n",
    "Los demás parámetros de inicialización no serán tenidos en cuenta, pues esos parámetros\n",
    "se tomarán del vectorizador cargado.\n",
    "'''\n",
    "v_bow_2 = VectorizadorFrecuencias(archivo_modelo='out/v_bow.pk')\n",
    "v_tfidf_2 = VectorizadorFrecuencias(archivo_modelo='out/v_tfidf.pk')\n",
    "\n",
    "# Se vectoriza el mismo texto con los vectorizadores cargados\n",
    "vector_bow_2 = v_bow_2.vectorizar(texto_nuevo, disperso=True)  # Salida como matriz dispersa\n",
    "vector_tfidf_2 = v_tfidf_2.vectorizar(texto_nuevo, disperso=False)  # Salida como un numpy array\n",
    "\n",
    "# Se comprueba que los vectores resultantes sean iguales\n",
    "np.all((vector_bow == vector_bow_2).toarray())\n",
    "np.all(vector_tfidf == vector_tfidf_2)\n",
    "\n",
    "### 2. Vectorizador por hashing ###\n",
    "\n",
    "## 2.1 Inicializar el vectorizador\n",
    "# Se define que los vectores tendrán 50 elementos\n",
    "v_hash = VectorizadorHash(n_elementos=50)\n",
    "\n",
    "## 2.2 Vectorizar textos utilizando el vectorizador\n",
    "'''\n",
    "A pesar de que el objeto HashingVectorizer de la librería scikit-learn\n",
    "incluye la función \"fit\", esta en la práctica no hace nada, pues este\n",
    "tipo de vectorizador funciona sin vocabulario y no debe ser ajustado.\n",
    "Por lo tanto, en la librería no se ha incluído esta opción. En vez de eso,\n",
    "el objeto se aplica directamente a los textos para vectorizarlos.\n",
    "\n",
    "Por este mismo motivo, el VectorizadorHash no permite la transformación\n",
    "inversa para identificar las palabras de un vector.\n",
    "'''\n",
    "vectores_prueba = v_hash.vectorizar(textos_prueba)\n",
    "vectores_prueba.shape\n",
    "\n",
    "vector_nuevo = v_hash.vectorizar(texto_nuevo, disperso=False)\n",
    "vector_nuevo.shape\n",
    "\n",
    "### 3. Vectorizador por Word2Vec ###\n",
    "\n",
    "## 3.1 Inicializar el vectorizador\n",
    "v_word2vec = VectorizadorWord2Vec()\n",
    "\n",
    "## 3.2 Vectorizar textos utilizando el vectorizador\n",
    "'''\n",
    "Este vectorizador obtiene los vectores de cada palabra de un texto, y luego las \n",
    "promedia para obtener un único vector para el texto completo.\n",
    "'''\n",
    "vector = v_word2vec.vectorizar(texto_nuevo)\n",
    "vector.shape\n",
    "\n",
    "## 3.3 Textos con palabras desconocidas (no incluídas en el modelo)\n",
    "'''\n",
    "El argumento booleano 'quitar_desconocidas' en la función vectorizar_texto hará que\n",
    "la función sea ligeramente más demorada, pero quizás más precisa, al no tener en cuenta palabras\n",
    "que no están incluídas en el modelo. Cuando este argumento es False (valor por defecto),\n",
    "para cada palabra desconocida se incluirá un vector de solo ceros, lo que afectará el vector \n",
    "promedio resultante.\n",
    "'''\n",
    "\n",
    "texto_1 = 'En este texto todas las palabras son conocidas, por lo que los resultados deberían ser iguales'\n",
    "texto_2 = 'En este texto hay asfafgf términos desconocidos FGs<g gsi<gi<sbf'\n",
    "\n",
    "for i, t in enumerate([texto_1, texto_2]):\n",
    "    print('\\n------------------')\n",
    "    print(f'Texto {i+1}:')\n",
    "    print(f'\"{t}\"')\n",
    "    v1 = v_word2vec.vectorizar(t, quitar_desconocidas=False)\n",
    "    v2 = v_word2vec.vectorizar(t, quitar_desconocidas=True)\n",
    "    print(f'Diferencia promedio: {(v1 - v2).mean()}')\n",
    "\n",
    "## 3.4 Obtener palabras y vectores de un texto\n",
    "df_palabras = v_word2vec.vectores_palabras(texto_nuevo, tipo='dataframe')\n",
    "dict_palabras = v_word2vec.vectores_palabras(texto_nuevo, tipo='diccionario')\n",
    "\n",
    "## 3.5 Similitudes entre textos\n",
    "'''\n",
    "Esta función aprovecha las facilidades de la librería Spacy para medir la\n",
    "similaridad entre 2 palabras o textos.\n",
    "'''\n",
    "t1 = 'los perros y los gatos suelen pelear mucho.'\n",
    "t2 = 'caninos y felinos entran en disputas con frecuencia.'\n",
    "t3 = 'este tercer texto habla sobre un tema distinto a los otros dos'\n",
    "\n",
    "for i in [t1, t2]:\n",
    "    for j in [t2, t3]:\n",
    "        if i != j:\n",
    "            similitud = v_word2vec.similitud_textos(i, j)\n",
    "            print('-----------------------')\n",
    "            print(f'Texto 1: {i}')\n",
    "            print(f'Texto 2: {j}')\n",
    "            print(f'Similitud entre textos: {similitud}')\n",
    "\n",
    "### 4. Vectorizador por Doc2Vec ###\n",
    "\n",
    "## 4.1 Inicializar el vectorizador\n",
    "# Se configura para que tenga 100 elementos y se entrene por 25 épocas\n",
    "'''\n",
    "Dado que el \"corpus\" de entrenamiento va a se pequeño (5 textos cortos), puede haber \n",
    "error si ningúna palabra cumple con el parámetro \"minima_cuenta=5\" (valor por defecto).\n",
    "Para evitar este error en este caso, se cambia ese parámetro a 1 (valor mínimo).\n",
    "'''\n",
    "v_doc2vec = VectorizadorDoc2Vec(n_elementos=100, epocas=25, minima_cuenta=1)\n",
    "\n",
    "## 4.2 Entrenar el modelo en un corpus\n",
    "v_doc2vec.entrenar_modelo(textos_limpios, archivo_salida='out/v_doc2vec.pk')\n",
    "\n",
    "## 4.3 Vectorizar textos utilizando el vectorizador\n",
    "vector = v_doc2vec.vectorizar(texto_nuevo)\n",
    "vector.shape\n",
    "\n",
    "## 4.4 Cargar un vectorizador entrenado previamente\n",
    "v_doc2vec_2 = VectorizadorDoc2Vec(archivo_modelo='out/v_doc2vec.pk')\n",
    "\n",
    "# Se vectoriza el mismo texto con el vectorizador cargado\n",
    "vector_2 = v_doc2vec_2.vectorizar(texto_nuevo)\n",
    "\n",
    "# Se comprueba que ambos vectores resultantes sean iguales\n",
    "np.all(vector == vector_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## 1. Funciones de limpieza de textos\n",
    "\n",
    "En esta sección se muestra cómo se pueden hacer distintos procesamientos de un texto de entrada para remover elementos como signos de puntuación, *stopwords*, números y acentos, que pueden llegar a entorpecer el análisis de un conjunto de documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Importar módulo de ConTexto y definir texto de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'contexto'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5a9cf379eec9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcontexto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlimpieza\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m texto_prueba = '''hola, esto es una prueba para verificar que la limpieza\n\u001b[0;32m      4\u001b[0m \u001b[0msea\u001b[0m \u001b[0mhecha\u001b[0m \u001b[0mcon\u001b[0m \u001b[0mprecisión\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mempeño\u001b[0m \u001b[0my\u001b[0m \u001b[0mcalidad\u001b[0m\u001b[0;31m!\u001b[0m \u001b[0mEsperamos\u001b[0m \u001b[0mque\u001b[0m \u001b[0mesté\u001b[0m \u001b[0mtodo\u001b[0m \u001b[0mde\u001b[0m \u001b[1;36m10.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'contexto'"
     ]
    }
   ],
   "source": [
    "from contexto.limpieza import *\n",
    "\n",
    "texto_prueba = '''hola, esto es una prueba para verificar que la limpieza\n",
    "sea hecha con precisión, empeño y calidad! Esperamos que esté todo de 10.\n",
    "\n",
    "Desde Amazonas hasta la Guajira y san andrés, desde John y María hasta Ernesto,\n",
    "esperamos       que todo funcione de manera correcta.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Aplicar funciones de limpieza de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza básica, se pasa todo a minúsculas y se eliminan signos de puntuación\n",
    "limpio_basico = limpieza_basica(texto_prueba)\n",
    "\n",
    "# Si se desea mantener los caracteres numéricos\n",
    "limpio_basico_nums = limpieza_basica(texto_prueba, quitar_numeros=False)\n",
    "\n",
    "# Para quitar acentos (diéresis, tildes y virgulillas)\n",
    "sin_acentos = remover_acentos(limpio_basico)\n",
    "\n",
    "# Quitar palabras con menos de 4 caracteres\n",
    "quitar_0a3_caracteres = remover_palabras_cortas(sin_acentos, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando la función `limpieza_texto` se puede, a la vez:\n",
    "\n",
    "* Pasar todo el texto a minúsculas\n",
    "* Quitar signos de puntuación\n",
    "* Quitar *stopwords* (palabras y/o expresiones). Para esto, se pueden pasar directamente las listas de palabras y expresiones a quitar, o se puede pasar un archivo que contenga esta información.\n",
    "* Quitar palabras de una longitud menor a *n* caracteres (configurable)\n",
    "* Quitar números (configurable)\n",
    "* Quitar acentos (configurable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limpio_completo = limpieza_texto(texto_prueba, ubicacion_archivo='in/stopwords_prueba.txt', n_min=3)\n",
    "print(limpio_completo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Quitar elementos repetidos de un texto\n",
    "\n",
    "La función `quitar_repetidos` permite quitar elementos repetidos de un texto, de acuerdo a un separador definido por el usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_repetido = 'hola, hola, como estas,hola, hola tu'\n",
    "\n",
    "# Aplicar función directamente\n",
    "quitar_repetidos(texto_repetido)\n",
    "\n",
    "# Especificar el separador entre documentos/frases\n",
    "quitar_repetidos(texto_repetido, ',')\n",
    "\n",
    "# Deshabilitar opción de quitar espacios al inicio y al final\n",
    "quitar_repetidos(texto_repetido, ',', remover_espacios=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Cargar listas de *stopwords*, predefinidas y definidas por el usuario\n",
    "\n",
    "**ConTexto** trae algunas listas predefinidas de *stopwords* que pueden ser cargadas y utilizadas directamente. Las listas incluidas son:\n",
    "\n",
    "* Palabras comunes del lenguaje castellano (solo palabras)\n",
    "* Nombres comunes de hombres y mujeres (solo palabras)\n",
    "* Nombres de municipios y departamentos de Colombia (palabras y expresiones: nombres compuestos como \"San Andrés\")\n",
    "\n",
    "Además de estas listas, la función `lista_stopwords` permite cargar listas predefinidas de las *stopwords* más comunes para varios lenguajes, utilizando la librería NLTK.\n",
    "\n",
    "Finalmente, la función `cargar_stopwords` permite al usuario cargar *stopwords* (tanto palabras como expresiones) desde un archivo plano. Las palabras/expresiones deben ir separadas por comas o ir en renglones separados para ser tenidas en cuenta por aparte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar listas de stopwords predefinidas\n",
    "nombres_hombres = lista_nombres('hombre')\n",
    "nombres_mujeres = lista_nombres('mujer')\n",
    "nombres_todos = lista_nombres()\n",
    "apellidos = lista_apellidos()\n",
    "municipios = lista_geo_colombia('municipios')\n",
    "departamentos = lista_geo_colombia('departamentos')\n",
    "todos_geo = lista_geo_colombia()\n",
    "\n",
    "# Stopwords comunes de varios lenguajes (por defecto se devuelven las de español)\n",
    "stopwords = lista_stopwords()\n",
    "stopwords_ingles = lista_stopwords('ingles')\n",
    "\n",
    "# Cargar archivo con lista de términos y expresiones que se desean remover\n",
    "custom_sw = cargar_stopwords('entrada/stopwords_prueba.txt')\n",
    "print(custom_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## 2. Funciones auxiliares para limpieza de textos\n",
    "\n",
    "Adicionalmente, el módulo auxiliar `limpieza_aux` contiene algunas funciones complementarias que permiten identificar y remover elementos adicionales que puedan entorpecer el análisis de un conjunto de textos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Importar funciones auxiliares y definir textos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contexto.utils.limpieza_aux import substrings_en_comun, detectar_coincidencias\n",
    "from contexto.utils.limpieza_aux import caracteres_repetidos, caracteres_consecutivos, consonantes_consecutivas\n",
    "from contexto.utils.limpieza_aux import quitar_coincidenias, quitar_palabras_atipicas\n",
    "\n",
    "# Corpus de prueba\n",
    "textos_prueba = [\n",
    "    'Este es el primer texto de prueba para la detección de coincidencias.',\n",
    "    'Una segunda oración permite evaluar si hay cadanea de caracteres elementos en común.',\n",
    "    'Tercera frase que consiste en un texto complementario con palabras comúnmente utilizadas.',\n",
    "    'En esta oración y la siguiente se introducen elementos para completar un grupo de por lo menos 5.',\n",
    "    'Finalmente, esta frase cierra un grupo de 5 oraciones para probar la detección de coincidencias.',\n",
    "    'Una última frase para ampliar un poco el grupo.']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Detectar y quitar coincidencias entre un conjunto de textos\n",
    "\n",
    "En ocasiones un documento puede tener un encabezado o pie de nota común en casi todas sus páginas. Esto puede entorpecer ciertos análisis, al darle un peso demasiado grande a estas coincidencias.\n",
    "\n",
    "Para evitar este problema, la función `quitar_coincidenias` (que a su vez utiliza las funciones `substrings_en_comun` y `detectar_coincidencias`) permite, para un conjunto de textos, encontrar y remover coincidencias (cadenas de caracteres) que cumplan una o varias de estas condiciones:\n",
    "\n",
    "* Que aparezcan en mínimo una proporción determinada de todos los textos\n",
    "* Que su longitud (cantidad de caracteres) sea mayor o igual a un valor determinado\n",
    "* Que la cadena tenga un número de palabras mayor o igual a un valor determinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'substrings_en_comun' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-0d3a08ea579c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Detectar coincidencias de por lo menos 4 y 10 caracteres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubstrings_en_comun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtextos_prueba\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtextos_prueba\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlongitud_min\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubstrings_en_comun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtextos_prueba\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtextos_prueba\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlongitud_min\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Detectar cadenas de caracteres de mínimo 2 palabras que estén en mínimo la mitad de los textos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'substrings_en_comun' is not defined"
     ]
    }
   ],
   "source": [
    "# Detectar coincidencias de por lo menos 4 y 10 caracteres\n",
    "print(substrings_en_comun(textos_prueba[4], textos_prueba[5], longitud_min=4))\n",
    "print(substrings_en_comun(textos_prueba[4], textos_prueba[5], longitud_min=10))\n",
    "\n",
    "# Detectar cadenas de caracteres de mínimo 2 palabras que estén en mínimo la mitad de los textos\n",
    "print(detectar_coincidencias(textos_prueba, prop=0.5, n_min=2, longitud_min=5))\n",
    "\n",
    "# Quitar las coincidencias encontradas\n",
    "print(quitar_coincidenias(textos_prueba, prop=0.5, n_min=2, longitud_min=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Detectar y quitar palabras o valores atípicos\n",
    "\n",
    "Si se está trabajando con un texto de mala calidad (por ejemplo, porque se aplicó OCR a un documento antiguo y mal escaneado), es posible que haya \"ruido\" en el texto, como palabras sin sentido, que puede afectar el análisis de este documento. Otro caso posible es trabajar con textos que tengan palabras o valores numéricos sospechosos (como \"abcde\" o \"0000000\"). En este caso, puede ser de utilidad poder detectar y/o remover estas palabras sospechosas o de ruido.\n",
    "\n",
    "Para evitar este problema, la función `quitar_palabras_atipicas` (que a su vez utiliza las funciones `caracteres_repetidos`, `caracteres_consecutivos` y `consonantes_consecutivas`) permite, para un conjunto de textos, encontrar y remover palabras que cumplan una o varias de estas condiciones:\n",
    "\n",
    "* Que tengan un número o letra repetidos de forma seguida más veces de lo permitido\n",
    "* Que tengan números o letras consecutivas de forma seguida en un número mayor de lo permitido\n",
    "* Que tengan más consonantes seguidas de lo permitido\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detectar si una palabra tiene una cantidad determinada de caracteres repetidos seguidos\n",
    "caracteres_repetidos('123444321', 4)\n",
    "# La función por defecto quita acentos y pasa todo a minúsculas, para que esto no afecte la búsqueda de repetidos\n",
    "caracteres_repetidos('GóOol', 3)\n",
    "\n",
    "# Detectar si una palabra tiene una cantidad determinada de caracteres consecutivos seguidos\n",
    "caracteres_consecutivos('123444321', 4)\n",
    "caracteres_consecutivos('aBCdE', 4)\n",
    "\n",
    "# Detectar si una palabra tiene una cantidad determinada de consonantes seguidas\n",
    "consonantes_consecutivas('AbStracto', 3)\n",
    "consonantes_consecutivas('Lynyrd Skynyrd', 4)\n",
    "# El resultado cambia si se deja de incluir la letra \"Y\" como vocal\n",
    "consonantes_consecutivas('Lynyrd Skynyrd', 4, incluir_y=False)\n",
    "# La función quita acentos por defecto, por lo que puede trabajar con consonantes que tengan algún tipo de acento o tilde\n",
    "consonantes_consecutivas('mñçs', 4)\n",
    "\n",
    "# Prueba de quitar palabras con problemas en un texto\n",
    "texto_prueba = 'HolaAá! esta es una pruebba para ver si, En 12345, se pueden abstraer las reglas del abcdario.'\n",
    "\n",
    "texto_sin_atipicas = quitar_palabras_atipicas(texto_prueba, n_repetidas=3, n_consecutivas=3, n_consonantes=4)\n",
    "\n",
    "print(f\"---------------\\nTexto original:\\n{texto_prueba}\")\n",
    "print(f\"---------------\\nTexto sin palabras detectadas como atípicas:\\n{texto_sin_atipicas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
